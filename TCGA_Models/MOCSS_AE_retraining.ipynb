{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a76c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1531904c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, random_split\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "from torchmetrics.classification import F1Score\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.cluster import KMeans,DBSCAN,Birch\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, v_measure_score\n",
    "\n",
    "sys.path.insert(1, '/home/wangc90/Data_integration/MOCSS/mocss/code/')\n",
    "from critic import LinearCritic\n",
    "# from Supcon import SupConLoss\n",
    "from contrastive_loss import InstanceLoss, ClusterLoss\n",
    "import evaluation\n",
    "from sklearn import metrics\n",
    "from Data_prep import DataSet_Prep, DataSet_construction\n",
    "random.seed(2023)\n",
    "torch.manual_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84d4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "# setup_seed(3)\n",
    "\n",
    "\n",
    "class SharedAndSpecificLoss(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SharedAndSpecificLoss, self).__init__()\n",
    "\n",
    "    ### The orthogonal loss defined here for shared and specific embeddings are\n",
    "    ### essentially the dot product of each correspoinding features between\n",
    "    ## shared embedding and specific embedding followed by taking the average \n",
    "    \n",
    "    ### shared: 100 X 1024\n",
    "    ### specific: 100 X 1024 \n",
    "    ### torch.mul(shared, specific) results in 100 X 1024 (element-wise product between these two matrix)\n",
    "    ### is the same as the dot product of embedding from shared and specific for each row (1024)\n",
    "    ### same as the shared X specific^t then take the average of the diagnol entries the the resulting 100 X 100 matrix \n",
    "    @staticmethod\n",
    "    def orthogonal_loss(shared, specific):\n",
    "#         shared = shared - shared.mean()\n",
    "#         specific = specific - specific.mean()\n",
    "        shared = F.normalize(shared, p=2, dim=1)\n",
    "        specific = F.normalize(specific, p=2, dim=1)\n",
    "        correlation_matrix = torch.mul(shared, specific)\n",
    "        cost = correlation_matrix.mean()\n",
    "        return cost\n",
    "\n",
    "    @staticmethod\n",
    "    def contrastive_loss(shared_1, shared_2, temperature, batch_size):\n",
    "        assert (shared_1.dim() == 2)\n",
    "        assert (shared_2.dim() == 2)\n",
    "#         shared_1 = shared_1 - shared_1.mean()\n",
    "#         shared_2 = shared_2 - shared_2.mean()\n",
    "        shared_1 = F.normalize(shared_1, p=2, dim=1)\n",
    "        shared_2 = F.normalize(shared_2, p=2, dim=1)\n",
    "\n",
    "        #Contrastive loss version1\n",
    "        criterion_instance = InstanceLoss(batch_size=batch_size, temperature=temperature)\n",
    "        loss = criterion_instance(shared_1, shared_2)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruction_loss(rec, ori):\n",
    "        assert (rec.dim() == 2)\n",
    "        assert (ori.dim() == 2)\n",
    "#         rec = rec - rec.mean()\n",
    "#         ori = ori - ori.mean()\n",
    "        rec = F.normalize(rec, p=2, dim=1)\n",
    "        ori = F.normalize(ori, p=2, dim=1)\n",
    "        \n",
    "        ## this is the forbenius norm of the normalized difference between\n",
    "        ## the reconstructed input and the original input\n",
    "        loss = torch.linalg.matrix_norm(rec-ori) \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def forward(self, shared1_output, shared2_output, specific1_output, specific2_output,\n",
    "                shared1_rec, shared2_rec, specific1_rec, specific2_rec,\n",
    "                ori1, ori2, shared1_mlp,shared2_mlp,temperature, batch_size):\n",
    "        \n",
    "        # orthogonal restrict\n",
    "        orthogonal_loss1 = self.orthogonal_loss(shared1_output, specific1_output)\n",
    "        orthogonal_loss2 = self.orthogonal_loss(shared2_output, specific2_output)\n",
    "        orthogonal_loss_all =  orthogonal_loss1 +   orthogonal_loss2 \n",
    "\n",
    "        # Contrastive Loss\n",
    "        contrastive_loss1 = self.contrastive_loss(shared1_mlp, shared2_mlp, temperature, batch_size)\n",
    "        contrastive_loss_all =  contrastive_loss1\n",
    "        # print(contrastive_loss_all)\n",
    "\n",
    "        # reconstruction Loss\n",
    "        reconstruction_loss1 = self.reconstruction_loss(shared1_rec, ori1) + self.reconstruction_loss(specific1_rec, ori1)\n",
    "        reconstruction_loss2 = self.reconstruction_loss(shared2_rec, ori2) + self.reconstruction_loss(specific2_rec, ori2)\n",
    "        reconstruction_loss_all =  reconstruction_loss1 +  reconstruction_loss2 \n",
    "        # print(reconstruction_loss_all)\n",
    "\n",
    "        ###################\n",
    "        # the reconstruction loss is weigthed by 0.7\n",
    "\n",
    "        ###################\n",
    "        \n",
    "\n",
    "        return orthogonal_loss_all, contrastive_loss_all, reconstruction_loss_all\n",
    "\n",
    "    \n",
    "    \n",
    "class SharedAndSpecificEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        ### embeding layers have the same dimensions for both shared and specific AE for all views\n",
    "        \n",
    "        view_size=[20531, 1046]\n",
    "        \n",
    "        n_units_1 = [0, 0, 0, 0]\n",
    "        n_units_1[0] = 1024\n",
    "        n_units_1[1] = 1024\n",
    "        n_units_1[2] = 32\n",
    "        n_units_1[3] = 32\n",
    "        \n",
    "        n_units_2 = n_units_1.copy()\n",
    "        \n",
    "        \n",
    "        mlp_size = [0, 0, 0, 0]\n",
    "        mlp_size[0] = 64\n",
    "        mlp_size[1] = 512\n",
    "        \n",
    "        \n",
    "        super(SharedAndSpecificEmbedding, self).__init__()\n",
    "        # View1\n",
    "        self.shared1_l1 = nn.Linear(view_size[0], n_units_1[0])\n",
    "        \n",
    "        self.shared1_l2 = nn.Linear(n_units_1[0], n_units_1[1])\n",
    "        self.shared1_l3 = nn.Linear(n_units_1[1], n_units_1[2])\n",
    "        self.shared1_l4 = nn.Linear(n_units_1[2], n_units_1[3])\n",
    "        \n",
    "        self.shared1_l3_ = nn.Linear(n_units_1[3], n_units_1[2])\n",
    "        self.shared1_l2_ = nn.Linear(n_units_1[2], n_units_1[1])\n",
    "        self.shared1_l1_ = nn.Linear(n_units_1[1], n_units_1[0])\n",
    "        \n",
    "        self.shared1_rec = nn.Linear(n_units_1[0], view_size[0])\n",
    "        \n",
    "        \n",
    "\n",
    "        self.specific1_l1 = nn.Linear(view_size[0], n_units_1[0])\n",
    "        self.specific1_l2 = nn.Linear(n_units_1[0], n_units_1[1])\n",
    "        self.specific1_l3 = nn.Linear(n_units_1[1], n_units_1[2])\n",
    "        self.specific1_l4 = nn.Linear(n_units_1[2], n_units_1[3])\n",
    "        \n",
    "        self.specific1_l3_ = nn.Linear(n_units_1[3], n_units_1[2])\n",
    "        self.specific1_l2_ = nn.Linear(n_units_1[2], n_units_1[1])\n",
    "        self.specific1_l1_ = nn.Linear(n_units_1[1], n_units_1[0])\n",
    "        self.specific1_rec = nn.Linear(n_units_1[0], view_size[0])\n",
    "        \n",
    "\n",
    "        self.view1_mlp1 = nn.Linear(n_units_1[3], mlp_size[0])\n",
    "        \n",
    "        self.view1_mlp2 = nn.Linear(mlp_size[0], mlp_size[1])\n",
    "\n",
    "        # View2\n",
    "        self.shared2_l1 = nn.Linear(view_size[1], n_units_2[0])\n",
    "        self.shared2_l2 = nn.Linear(n_units_2[0], n_units_2[1])\n",
    "        self.shared2_l3 = nn.Linear(n_units_2[1], n_units_2[2])\n",
    "        self.shared2_l4 = nn.Linear(n_units_2[2], n_units_2[3])\n",
    "        \n",
    "        self.shared2_l3_ = nn.Linear(n_units_2[3], n_units_2[2])\n",
    "        self.shared2_l2_ = nn.Linear(n_units_2[2], n_units_2[1])\n",
    "        self.shared2_l1_ = nn.Linear(n_units_2[1], n_units_2[0])\n",
    "        self.shared2_rec = nn.Linear(n_units_2[0], view_size[1])\n",
    "\n",
    "        self.specific2_l1 = nn.Linear(view_size[1], n_units_2[0])\n",
    "        self.specific2_l2 = nn.Linear(n_units_2[0], n_units_2[1])\n",
    "        self.specific2_l3 = nn.Linear(n_units_2[1], n_units_2[2])\n",
    "        self.specific2_l4 = nn.Linear(n_units_2[2], n_units_2[3])\n",
    "        \n",
    "        self.specific2_l3_ = nn.Linear(n_units_2[3], n_units_2[2])\n",
    "        self.specific2_l2_ = nn.Linear(n_units_2[2], n_units_2[1])\n",
    "        self.specific2_l1_ = nn.Linear(n_units_2[1], n_units_2[0])\n",
    "        self.specific2_rec = nn.Linear(n_units_2[0], view_size[1])\n",
    "\n",
    "        self.view2_mlp1 = nn.Linear(n_units_2[3], mlp_size[0])\n",
    "        self.view2_mlp2 = nn.Linear(mlp_size[0], mlp_size[1])\n",
    "        \n",
    "\n",
    "#         # Init weight\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         init.kaiming_normal_(self.shared1_l1.weight)\n",
    "#         init.kaiming_normal_(self.shared1_l2.weight)\n",
    "#         init.kaiming_normal_(self.shared1_l3.weight)\n",
    "#         init.kaiming_normal_(self.shared1_l4.weight)\n",
    "        \n",
    "#         init.kaiming_normal_(self.shared1_l3_.weight)\n",
    "#         init.kaiming_normal_(self.shared1_l2_.weight)\n",
    "#         init.kaiming_normal_(self.shared1_l1_.weight)\n",
    "#         init.kaiming_normal_(self.shared1_rec.weight)\n",
    "\n",
    "#         init.kaiming_normal_(self.specific1_l1.weight)\n",
    "#         init.kaiming_normal_(self.specific1_l2.weight)\n",
    "#         init.kaiming_normal_(self.specific1_l3.weight)\n",
    "#         init.kaiming_normal_(self.specific1_l4.weight)\n",
    "        \n",
    "#         init.kaiming_normal_(self.specific1_l3_.weight)\n",
    "#         init.kaiming_normal_(self.specific1_l2_.weight)\n",
    "#         init.kaiming_normal_(self.specific1_l1_.weight)\n",
    "#         init.kaiming_normal_(self.specific1_rec.weight)\n",
    "\n",
    "#         init.kaiming_normal_(self.shared2_l1.weight)\n",
    "#         init.kaiming_normal_(self.shared2_l2.weight)\n",
    "#         init.kaiming_normal_(self.shared2_l3.weight)\n",
    "#         init.kaiming_normal_(self.shared2_l4.weight)\n",
    "        \n",
    "#         init.kaiming_normal_(self.shared2_l3_.weight)\n",
    "#         init.kaiming_normal_(self.shared2_l2_.weight)\n",
    "#         init.kaiming_normal_(self.shared2_l1_.weight)\n",
    "#         init.kaiming_normal_(self.shared2_rec.weight)\n",
    "\n",
    "#         init.kaiming_normal_(self.specific2_l1.weight)\n",
    "#         init.kaiming_normal_(self.specific2_l2.weight)\n",
    "#         init.kaiming_normal_(self.specific2_l3.weight)\n",
    "#         init.kaiming_normal_(self.specific2_l4.weight)\n",
    "        \n",
    "#         init.kaiming_normal_(self.specific2_l3_.weight)\n",
    "#         init.kaiming_normal_(self.specific2_l2_.weight)\n",
    "#         init.kaiming_normal_(self.specific2_l1_.weight)\n",
    "#         init.kaiming_normal_(self.specific2_rec.weight)\n",
    "\n",
    "\n",
    "#         init.kaiming_normal_(self.view1_mlp1.weight)\n",
    "#         init.kaiming_normal_(self.view1_mlp2.weight)\n",
    "#         init.kaiming_normal_(self.view2_mlp1.weight)\n",
    "#         init.kaiming_normal_(self.view2_mlp2.weight)\n",
    "\n",
    "    def forward(self, view1_input, view2_input, label):\n",
    "        # View1\n",
    "        view1_specific = F.tanh(self.specific1_l1(view1_input))\n",
    "        view1_specific = F.tanh(self.specific1_l2(view1_specific))\n",
    "        view1_specific = F.tanh(self.specific1_l3(view1_specific))\n",
    "        view1_specific_em = F.tanh(self.specific1_l4(view1_specific))\n",
    "        \n",
    "        view1_specific = F.tanh(self.specific1_l3_(view1_specific_em))\n",
    "        view1_specific = F.tanh(self.specific1_l2_(view1_specific))\n",
    "        view1_specific = F.tanh(self.specific1_l1_(view1_specific))\n",
    "        view1_specific_rec = torch.sigmoid(self.specific1_rec(view1_specific))\n",
    "\n",
    "        view1_shared = F.tanh(self.shared1_l1(view1_input))\n",
    "        view1_shared = F.tanh(self.shared1_l2(view1_shared))\n",
    "        view1_shared = F.tanh(self.shared1_l3(view1_shared))\n",
    "        view1_shared_em = F.tanh(self.shared1_l4(view1_shared))\n",
    "        \n",
    "        view1_shared = F.tanh(self.shared1_l3_(view1_shared_em))\n",
    "        view1_shared = F.tanh(self.shared1_l2_(view1_shared))\n",
    "        view1_shared = F.tanh(self.shared1_l1_(view1_shared))\n",
    "        view1_shared_rec = torch.sigmoid(self.shared1_rec(view1_shared))\n",
    "\n",
    "        view1_shared_mlp = F.tanh(self.view1_mlp1(view1_shared_em))\n",
    "        view1_shared_mlp = F.tanh(self.view1_mlp2(view1_shared_mlp))\n",
    "\n",
    "        # View2\n",
    "        view2_specific = F.tanh(self.specific2_l1(view2_input))\n",
    "        view2_specific = F.tanh(self.specific2_l2(view2_specific))\n",
    "        view2_specific = F.tanh(self.specific2_l3(view2_specific))\n",
    "        view2_specific_em = F.tanh(self.specific2_l4(view2_specific))\n",
    "        view2_specific = F.tanh(self.specific2_l3_(view2_specific_em))\n",
    "        view2_specific = F.tanh(self.specific2_l2_(view2_specific))\n",
    "        view2_specific = F.tanh(self.specific2_l1_(view2_specific))\n",
    "        view2_specific_rec = torch.sigmoid(self.specific2_rec(view2_specific))\n",
    "\n",
    "        view2_shared = F.tanh(self.shared2_l1(view2_input))\n",
    "        view2_shared = F.tanh(self.shared2_l2(view2_shared))\n",
    "        view2_shared = F.tanh(self.shared2_l3(view2_shared))\n",
    "        view2_shared_em = F.tanh(self.shared2_l4(view2_shared))\n",
    "        view2_shared = F.tanh(self.shared2_l3_(view2_shared_em))\n",
    "        view2_shared = F.tanh(self.shared2_l2_(view2_shared))\n",
    "        view2_shared = F.tanh(self.shared2_l1_(view2_shared))\n",
    "        view2_shared_rec = torch.sigmoid(self.shared2_rec(view2_shared))\n",
    "\n",
    "        view2_shared_mlp = F.tanh(self.view2_mlp1(view2_shared_em))\n",
    "        view2_shared_mlp = F.tanh(self.view2_mlp2(view2_shared_mlp))\n",
    "\n",
    "\n",
    "        return view1_specific_em, view1_shared_em,\\\n",
    "                view2_specific_em, view2_shared_em,\\\n",
    "                view1_specific_rec, view1_shared_rec,\\\n",
    "                view2_specific_rec, view2_shared_rec,\\\n",
    "               view1_shared_mlp, view2_shared_mlp, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f515b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSO_AE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec37dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### assuming all the hyperparameters are optimized by Optuna and then retrain the model on the entire training set\n",
    "\n",
    "def retraining(model, dataset, model_folder):\n",
    "    \n",
    "    train_recon_loss_ = []\n",
    "\n",
    "    device = torch.device('cuda:1') if torch. cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    batch_size = 512\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "#     print(len(train_loader))\n",
    "    \n",
    "    model = model().to(device=device)\n",
    "#     print(model)\n",
    "    \n",
    "    \n",
    "    optimizer_name = 'Adam'\n",
    "    lr = 0.0005606116734930229\n",
    "    l2_lambda = 1.3749522904352792e-07\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "    \n",
    "    epochs = 120 ### reduce the epochs from 150 to 100 to reduce the potential overfitting\n",
    "    \n",
    "    ortho_multiplier = 0.01\n",
    "    for epoch in range(epochs):\n",
    "        #         print(f\"I'am in the epoch {epoch}\")\n",
    "        model.train()\n",
    "        # record the training loss\n",
    "        total_recon_loss = 0.0\n",
    "        total_train = 0.0\n",
    "\n",
    "        ## deal with different number of features in different dataset with star* notation\n",
    "        for view1_train_data, view2_train_data, train_labels in train_loader:\n",
    "            ### this line is just for nn.CrossEntropy loss otherwise can be safely removed\n",
    "            view1_train_data = view1_train_data.type(torch.float32).to(device)\n",
    "            view2_train_data = view2_train_data.type(torch.float32).to(device)\n",
    "            train_labels = train_labels.type(torch.LongTensor).to(device)\n",
    "\n",
    "            view1_specific_em, view1_shared_em, view2_specific_em, view2_shared_em,\\\n",
    "            view1_specific_rec, view1_shared_rec, view2_specific_rec, view2_shared_rec,\\\n",
    "            view1_shared_mlp, view2_shared_mlp, train_labels = model(view1_train_data, view2_train_data, train_labels)\n",
    "            \n",
    "            train_size = view1_specific_em.size()[0]\n",
    "            \n",
    "            loss_function = SharedAndSpecificLoss()\n",
    "            ortho_loss, contrastive_loss, recon_loss = loss_function(shared1_output=view1_shared_em,\\\n",
    "                                                                     shared2_output=view2_shared_em,\\\n",
    "                                                                     specific1_output = view1_specific_em,\\\n",
    "                                                                     specific2_output = view2_specific_em,\\\n",
    "                                                                     shared1_rec = view1_shared_rec,\\\n",
    "                                                                     specific1_rec = view1_specific_rec,\\\n",
    "                                                                     shared2_rec=view2_shared_rec,\\\n",
    "                                                                     specific2_rec=view2_specific_rec,\\\n",
    "                                                                     ori1 = view1_train_data,\\\n",
    "                                                                     ori2 = view2_train_data,\\\n",
    "                                                                     shared1_mlp=view1_shared_mlp,\\\n",
    "                                                                     shared2_mlp=view2_shared_mlp,\\\n",
    "                                                                     batch_size = view1_shared_em.shape[0],\\\n",
    "                                                                     temperature = 0.4)\n",
    "            \n",
    "            loss = ortho_loss + contrastive_loss + (ortho_multiplier * recon_loss)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()  # empty the gradient from last round\n",
    "\n",
    "            # calculate the gradient\n",
    "            loss.backward()\n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train += train_size\n",
    "            \n",
    "            total_recon_loss += recon_loss.item()\n",
    "    \n",
    "        train_recon_loss_.append(total_recon_loss / total_train)\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'finished retraining on epoch: {epoch}')\n",
    "    # save the model at the end of 150 epochs\n",
    "    model_path = f\"{model_folder}/retrained_model_{epoch}.pt\"\n",
    "    \n",
    "    torch.save(model, model_path)\n",
    "    \n",
    "    return train_recon_loss_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5e05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MOCSS_retraining():\n",
    "    ### where to save the 3-fold CV validation acc\n",
    "\n",
    "    ### where to save the retrained model\n",
    "    model_folder = '/home/wangc90/Data_integration/TCGA_model_outputs/model_retraining_outputs/MOCSS_AE_retraining'\n",
    "\n",
    "    combined_exp_df = pd.read_csv('/home/wangc90/Data_integration/TCGA_data/TCGA_primary_tumor_data/combined_exp_df.csv', sep='\\t')\n",
    "    combined_miRNA_df = pd.read_csv('/home/wangc90/Data_integration/TCGA_data/TCGA_primary_tumor_data/combined_miRNA_df.csv', sep='\\t')\n",
    "\n",
    "    labels = pd.read_csv('/home/wangc90/Data_integration/TCGA_data/TCGA_primary_tumor_data/labels.csv', sep='\\t')['0']\n",
    "\n",
    "    dataset_prep = DataSet_Prep(data1=combined_exp_df, data2=combined_miRNA_df, label=labels, training_prop=0.8)\n",
    "\n",
    "    train_key, test_key = dataset_prep.get_train_test_keys()\n",
    "\n",
    "    feature1_tensors, feature2_tensors, label_tensors = dataset_prep.to_tensor(train_key)\n",
    "    \n",
    "\n",
    "    train_dataset = DataSet_construction(feature1_tensors, feature2_tensors, label_tensors)\n",
    "\n",
    "    print(len(train_dataset))\n",
    "\n",
    "    train_recon_loss_ = retraining(model=SharedAndSpecificEmbedding, dataset=train_dataset,model_folder=model_folder)\n",
    "    \n",
    "    return train_recon_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456abea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature1 and feature2 are being scaled with MinMaxScaler\n",
      "1494\n",
      "finished retraining on epoch: 9\n",
      "finished retraining on epoch: 19\n",
      "finished retraining on epoch: 29\n",
      "finished retraining on epoch: 39\n",
      "finished retraining on epoch: 49\n",
      "finished retraining on epoch: 59\n",
      "finished retraining on epoch: 69\n",
      "finished retraining on epoch: 79\n",
      "finished retraining on epoch: 89\n",
      "finished retraining on epoch: 99\n",
      "finished retraining on epoch: 109\n"
     ]
    }
   ],
   "source": [
    "train_recon_loss_ = MOCSS_retraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8be3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed74a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_recon_loss_, label='train_recon_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b3d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
